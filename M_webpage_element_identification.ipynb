{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsRG1DbajIl7WXfM28yHk2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rad-Monk/TF-Notebooks/blob/main/M_webpage_element_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ECTWKxsga9BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mGUFKsjUeteM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/ui_elements_datasets\""
      ],
      "metadata": {
        "id": "6PvdgsPCevBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "id": "ZvHb-AIsfTdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_numerical = {\n",
        "    \"main_breadcrumbs\": 0,\n",
        "    \"main_icons\": 1,\n",
        "    \"main_container\": 2,\n",
        "    \"main_modals\": 3,\n",
        "    \"main_logos\": 4,\n",
        "    \"main_text\": 5,\n",
        "    \"main_input_fields\": 6,\n",
        "    \"main_radio\": 7\n",
        "}"
      ],
      "metadata": {
        "id": "S0JqBrx1f3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_categorical = {\n",
        "    0: \"breadcrumbs\",\n",
        "    1: \"icons\",\n",
        "    2: \"container\",\n",
        "    3: \"modals\",\n",
        "    4: \"logos\",\n",
        "    5: \"text\",\n",
        "    6: \"input fields\",\n",
        "    7: \"main_radio\"\n",
        "}"
      ],
      "metadata": {
        "id": "CIqRSUefg0O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_generator = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 10,\n",
        "    width_shift_range = .5,\n",
        "    height_shift_range = .5,\n",
        "    shear_range = .1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = False,\n",
        "    brightness_range = [.7, 1.3],\n",
        "    channel_shift_range = 100.,\n",
        "    zoom_range = .1\n",
        ")\n",
        "rescale_generator = ImageDataGenerator(\n",
        "    rescale = 1./255\n",
        ")"
      ],
      "metadata": {
        "id": "_D5Ln9ojj_Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in os.listdir(dataset_dir):\n",
        "  folder_path = os.path.join(dataset_dir, folder)\n",
        "  print(len(os.listdir(folder_path)), folder)"
      ],
      "metadata": {
        "id": "sfKTi6hqeV97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "class_dir = '/content/drive/MyDrive/ui_elements_datasets/main_icons'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(class_dir)\n",
        "print(\"Found files:\", files)\n",
        "\n",
        "# Sample image file path\n",
        "image_path = os.path.join(class_dir, files[0])  # Assuming there is at least one file\n",
        "try:\n",
        "    img = Image.open(image_path)\n",
        "    img.show()  # This will display the image if possible\n",
        "    print(\"Image loaded successfully!\")\n",
        "except IOError:\n",
        "    print(\"Image could not be opened!\")"
      ],
      "metadata": {
        "id": "Bj-FQxap-wlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_generator_batch_size(folder,folder_path,prefix,batch_size, tot_img):\n",
        "  image_gen = image_generator.flow_from_directory(\n",
        "        f'{dataset_dir}/',\n",
        "        classes = [folder],\n",
        "        batch_size = batch_size,\n",
        "        target_size = (224,224),\n",
        "        save_to_dir = folder_path,\n",
        "        save_prefix = prefix,\n",
        "        save_format = \"png\",\n",
        "        class_mode = None\n",
        "        )\n",
        "  for _ in range(tot_img):\n",
        "    image_gen.next()\n",
        "\n",
        "def rescale_generator_batch_size(folder,folder_path, batch_size, prefix, tot_img ):\n",
        "      rescale_flow = rescale_generator.flow_from_directory(\n",
        "        f'{dataset_dir}/',\n",
        "        classes = [folder],\n",
        "        batch_size = tot_img,\n",
        "        target_size = (224,224),\n",
        "        save_to_dir = folder_path,\n",
        "        save_prefix = prefix,\n",
        "        save_format = \"png\",\n",
        "        class_mode = None\n",
        "        )\n",
        "      rescale_flow.next()\n",
        "\n",
        "\n",
        "\n",
        "#for folder in os.listdir(dataset_dir):\n",
        " # if folder in labels_numerical:\n",
        "  #  folder_path = os.path.join(dataset_dir, folder)\n",
        "#\n",
        " #   tot_img = len(os.listdir(folder_path))\n",
        "#\n",
        " #   if folder == \"main_breadcrumbs\":\n",
        "  #    image_generator_batch_size(folder,folder_path,\"aug_\", 2, tot_img*16)\n",
        "   # if folder == \"main_container\":\n",
        "    #  image_generator_batch_size(folder,folder_path,\"aug_\", 2, tot_img*73)\n",
        "    #if folder == \"main_icons\":\n",
        "    #  rescale_generator_batch_size(folder,folder_path, 1, \"\", tot_img)\n",
        "    #if folder == \"main_input_fields\":\n",
        "    #  image_generator_batch_size(folder,folder_path,\"\", tot_img, 1)\n",
        "    #if folder == \"main_logos\":\n",
        "    #  rescale_generator_batch_size(folder ,folder_path, 1, \"aug_\", tot_img)\n",
        "    #if folder == \"main_modals\":\n",
        "    #  image_generator_batch_size(folder,folder_path,\"aug_\", 24, tot_img)\n",
        "    #if folder == \"main_radio\":\n",
        "    #  image_generator_batch_size(folder,folder_path,\"aug_\", 32, tot_img )\n",
        "    #if folder == \"main_text\":\n",
        "    #  image_generator_batch_size(folder,folder_path, \"aug_\",12, tot_img)\n",
        "\n"
      ],
      "metadata": {
        "id": "UHoh4Gz5hF-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def breadcrumb():\n",
        "  folder_icon_path = os.path.join(dataset_dir, \"main_breadcrumbs\")\n",
        "  icons = os.listdir(folder_icon_path)\n",
        "  tot_icons = len(icons)\n",
        "  images_to_remove = tot_icons // 2\n",
        "\n",
        "  random.shuffle(icons)\n",
        "  files_to_delete = icons[:200]\n",
        "\n",
        "  for filename in files_to_delete:\n",
        "    file_path = os.path.join(folder_icon_path, filename)\n",
        "    os.remove(file_path)\n",
        "\n",
        "def icons():\n",
        "  folder_icon_path = os.path.join(dataset_dir, \"main_icons\")\n",
        "  icons = os.listdir(folder_icon_path)\n",
        "  tot_icons = len(icons)\n",
        "  images_to_remove = tot_icons // 2\n",
        "\n",
        "  random.shuffle(icons)\n",
        "  files_to_delete = icons[:200]\n",
        "\n",
        "  for filename in files_to_delete:\n",
        "    file_path = os.path.join(folder_icon_path, filename)\n",
        "    os.remove(file_path)\n",
        "\n",
        "def input_fields():\n",
        "  folder_icon_path = os.path.join(dataset_dir, \"main_input_fields\")\n",
        "  icons = os.listdir(folder_icon_path)\n",
        "  tot_icons = len(icons)\n",
        "  images_to_remove = tot_icons // 2\n",
        "\n",
        "  random.shuffle(icons)\n",
        "  files_to_delete = icons[:200]\n",
        "\n",
        "  for filename in files_to_delete:\n",
        "    file_path = os.path.join(folder_icon_path, filename)\n",
        "    os.remove(file_path)"
      ],
      "metadata": {
        "id": "UMBJkTBik9m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/dataset\"\n",
        "os.makedirs(dataset_path, exist_ok = True)"
      ],
      "metadata": {
        "id": "JQeEuCdZ0syX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(f\"{dataset_path}/train\", exist_ok = True)\n",
        "os.makedirs(f\"{dataset_path}/val\", exist_ok = True)\n",
        "os.makedirs(f\"{dataset_path}/test\", exist_ok = True)"
      ],
      "metadata": {
        "id": "k9hts5az4g8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_path = os.path.join(dataset_path, \"train\")\n",
        "val_ds_path = os.path.join(dataset_path, \"val\")\n",
        "test_ds_path = os.path.join(dataset_path, \"test\")"
      ],
      "metadata": {
        "id": "c0Xn4NGr4Adb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_path)"
      ],
      "metadata": {
        "id": "zy0FguBq4VQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = [.7,.1,.2]\n",
        "\n",
        "for folder_name in os.listdir(dataset_dir):\n",
        "  folder_path = os.path.join(dataset_dir, folder_name)\n",
        "  files = os.listdir(folder_path)\n",
        "  random.shuffle(files)\n",
        "\n",
        "  start_train = 0\n",
        "  end_train = int(len(files) * ratios[0])\n",
        "  start_val = end_train\n",
        "  end_val = start_val + int(len(files) * ratios[1])\n",
        "\n",
        "  for i,filename in enumerate(files):\n",
        "    src_path = os.path.join(folder_path, filename)\n",
        "    if i >= start_train and i < end_train:\n",
        "      dest_path = os.path.join(train_ds_path, folder_name, filename)\n",
        "    elif i>= start_val and i< end_val:\n",
        "      dest_path = os.path.join(val_ds_path, folder_name, filename)\n",
        "    else:\n",
        "      dest_path = os.path.join(test_ds_path, folder_name, filename)\n",
        "\n",
        "    os.makedirs(os.path.dirname(dest_path), exist_ok = True)\n",
        "    shutil.copyfile(src_path, dest_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "bIv3vH-54bG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(f\"{train_ds_path}/main_icons\"))"
      ],
      "metadata": {
        "id": "rBX8-uud9Ja_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = f\"{train_ds_path}/main_icons/24-hours-fill.png\"\n",
        "image = Image.open(image_path)\n",
        "width, height = image.size\n",
        "print(f\"Image Scale (Width x Height): {width} x {height} pixels\")\n"
      ],
      "metadata": {
        "id": "kPUTGnY9OQly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import losses"
      ],
      "metadata": {
        "id": "Ow4CXlyAP1N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "target_size = (224,224)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_ds_path,\n",
        "    batch_size = batch_size,\n",
        "    target_size = target_size,\n",
        "    class_mode = \"sparse\"\n",
        ")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_ds_path,\n",
        "    batch_size = batch_size,\n",
        "    target_size = target_size,\n",
        "    class_mode = \"sparse\"\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_ds_path,\n",
        "    batch_size = batch_size,\n",
        "    target_size = target_size,\n",
        "    class_mode = \"sparse\"\n",
        ")"
      ],
      "metadata": {
        "id": "3xGM7NkoXTgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = test_generator"
      ],
      "metadata": {
        "id": "RcWqo61cnM9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_ds)"
      ],
      "metadata": {
        "id": "vjrzBdn5nQuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "def list_all_files(directory_path):\n",
        "  return glob.glob(f\"{directory_path}/**/*\", recursive = True)\n",
        "\n",
        "all_files = list_all_files(train_ds_path)\n",
        "train_images = [file for file in all_files if os.path.isfile(file)]\n",
        "\n",
        "print(len(train_images))"
      ],
      "metadata": {
        "id": "B9jvLdOd-nDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=target_size + (3,)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "J34s8pRVY3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = .001)\n",
        "model.compile(\n",
        "    loss = \"SparseCategoricalCrossentropy\",\n",
        "    metrics = \"accuracy\",\n",
        "    optimizer = optimizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "yuStkZ_dZTbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_dir = 'ui_detection_model'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch:04d}_batch_{batch:05d}\")\n",
        "\n",
        "class CustomCheckpoint(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, save_freq, checkpoint_prefix):\n",
        "    super(CustomCheckpoint, self).__init__()\n",
        "    self.save_freq = save_freq\n",
        "    self.checkpoint_prefix = checkpoint_prefix\n",
        "    self.batch = 0\n",
        "    self.epoch = 0\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch = epoch + 1\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs = None):\n",
        "    self.batch += 1\n",
        "    if self.batch % self.save_freq == 0:\n",
        "      checkpoint = tf.train.Checkpoint(model=self.model, optimizer=self.model.optimizer)\n",
        "      checkpoint.save(file_prefix=self.checkpoint_prefix.format(epoch=self.epoch, batch=self.batch))\n",
        "\n",
        "\n",
        "n_batch = math.ceil(len(train_images) / batch_size)\n",
        "\n",
        "def load_latest_checkpoint(model, optmizer, checkpoint_dir):\n",
        "  latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  if latest_checkpoint:\n",
        "    print(f\"loading checkpoint: {latest_checkpoint}....\")\n",
        "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "    checkpoint.restore(latest_checkpoint).expect_partial()\n",
        "    parts = latest_checkpoint.split('/')[1].split('_')\n",
        "    print(parts)\n",
        "    epoch = int(parts[1])\n",
        "    batch = int(parts[3][:-2])\n",
        "    print(f\"loaded checkpoint at epoch {epoch} and batch {batch}\")\n",
        "    return epoch, batch\n",
        "  else:\n",
        "    print(\"no checkpoint found, starting from scratch\")\n",
        "    return 1, 0\n",
        "\n",
        "intial_epoch, initial_batch = load_latest_checkpoint(model, optimizer, checkpoint_dir)\n",
        "\n",
        "custom_cp_callback = CustomCheckpoint(save_freq = 5, checkpoint_prefix = checkpoint_prefix)\n",
        "\n",
        "epoch = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    initial_epoch = intial_epoch - 1,\n",
        "    epochs = epoch,\n",
        "    validation_data = val_generator,\n",
        "    callbacks = [custom_cp_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "SFZFPZjB8m43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def delete_checkpoint_files():\n",
        "    checkpoint_pattern = \"/content/ui_detection_model/ckpt_0010*\"  # Pattern to match files starting with ckpt-0001 to ckpt-0006\n",
        "\n",
        "    # List all files matching the pattern\n",
        "    matching_files = glob.glob(checkpoint_pattern)\n",
        "\n",
        "    # Delete each matching file\n",
        "    for file in matching_files:\n",
        "        try:\n",
        "            os.remove(file)\n",
        "            print(f\"Deleted file: {file}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting file {file}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "delete_checkpoint_files()"
      ],
      "metadata": {
        "id": "SMmuzGDSaEaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_locally(model, save_path):\n",
        "    try:\n",
        "        # Save the model\n",
        "        model.save(save_path)\n",
        "        print(f\"Model saved successfully at: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {str(e)}\")"
      ],
      "metadata": {
        "id": "PUb4dOu-gj6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_locally(model, \"no_tf_ui_classifier\")"
      ],
      "metadata": {
        "id": "GHWCdvevbmv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss,acc= model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "KeSpiFcGbwZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "id": "J227OKG2nhbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxA3k5Pinj-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}