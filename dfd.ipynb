{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rad-Monk/TF-Notebooks/blob/main/dfd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLgw8Tx3XJgn"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rhCGsNHXTE2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSFFFUTcXf1p"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d ahmadawad732/uadfv-dataset-new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gDwaGwDXtN-"
      },
      "outputs": [],
      "source": [
        "!unzip uadfv-dataset-new.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpbBmdM2sfIU"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations\n",
        "!pip install cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDFf0RivJwqV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "from IPython.display import Image\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from albumentations import Compose, ImageCompression, GaussNoise, GaussianBlur, HorizontalFlip, PadIfNeeded, OneOf, RandomBrightnessContrast, FancyPCA, HueSaturationValue, ToGray, ShiftScaleRotate, Resize\n",
        "import albumentations.augmentations.functional as F\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulDo7u3SUM7F"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "MAX_SEQ_LENGTH = 250\n",
        "NUM_FEATURES = 1792 #for efficientnetb4\n",
        "# NUM_FEATURES = 2048 #for inceptionv3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlMOwL0pYZTw"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "### DATASET 1 (FaceForensics + Celeb)\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/fake_videos/'):\n",
        "    for filename in filenames:\n",
        "        if 'validation' in dirname:\n",
        "            val_data.append(os.path.join(dirname, filename))\n",
        "        elif 'train' in dirname:\n",
        "            train_data.append(os.path.join(dirname, filename))\n",
        "        else:\n",
        "            test_data.append(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HdwbNUcao_r"
      },
      "outputs": [],
      "source": [
        "train_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dx7Y0oza_Cs"
      },
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame(train_data)\n",
        "val_df = pd.DataFrame(val_data)\n",
        "test_df = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrCYqY2nbsU0"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxuuU2yxbP70"
      },
      "outputs": [],
      "source": [
        "def label_file(row):\n",
        "    if 'real' in row:\n",
        "        return \"real\"\n",
        "    elif 'fake' in row:\n",
        "        return \"fake\"\n",
        "    else:\n",
        "        return 'undefined'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CTamKqfbSqC"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df[0].apply(label_file)\n",
        "val_df['label'] = val_df[0].apply(label_file)\n",
        "test_df['label'] = test_df[0].apply(label_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42fJbefMbbfd"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMQXqm0_cGAO"
      },
      "outputs": [],
      "source": [
        "print(train_df['label'].value_counts())\n",
        "print(val_df['label'].value_counts())\n",
        "print(test_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt-gbLSBUTiR"
      },
      "outputs": [],
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cZB7R51YaHh"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "\n",
        "IMG_SIZE = 224  # Adjust as necessary for EfficientNetB4\n",
        "\n",
        "def build_feature_extractor():\n",
        "    # Create the base EfficientNetB4 model with pre-trained weights from ImageNet\n",
        "    feature_extractor = EfficientNetB4(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,  # Exclude the classification head\n",
        "        pooling=\"avg\",  # Use global average pooling to get a feature vector\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "\n",
        "    # Define the model's input\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # The EfficientNet model will automatically handle the input preprocessing\n",
        "    outputs = feature_extractor(inputs)\n",
        "\n",
        "    # Create a Keras Model with the specified input and output\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "# Instantiate the feature extractor\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "\n",
        "# def build_feature_extractor():\n",
        "#     feature_extractor = keras.applications.InceptionV3(\n",
        "#         weights=\"imagenet\",\n",
        "#         include_top=False,\n",
        "#         pooling=\"avg\",\n",
        "#         input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "#     )\n",
        "#     preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "#     preprocessed = preprocess_input(inputs)\n",
        "\n",
        "#     outputs = feature_extractor(preprocessed)\n",
        "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "# feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pprwv_nZcgrJ"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAMZiC-VjoZa"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-h_mbsTdryK"
      },
      "outputs": [],
      "source": [
        "train_df.rename(columns={0:'filepath'}, inplace=True)\n",
        "test_df.rename(columns={0:'filepath'}, inplace=True)\n",
        "val_df.rename(columns={0:'filepath'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igcVAqA_cwMN"
      },
      "outputs": [],
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq4AJ5P7dyiS"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Dy0rQ5kDx7"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXJdXIwAtayN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def IsotropicResize(image, size=380):\n",
        "#     # Compute the scaling factor to resize the image while preserving the aspect ratio\n",
        "#     h, w = image.shape[:2]\n",
        "#     scale = size / max(h, w)\n",
        "#     resized_image = cv2.resize(image, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "#     # Pad the image if necessary to ensure it's square\n",
        "#     if h > w:\n",
        "#         pad_w = (size - resized_image.shape[1]) // 2\n",
        "#         pad_h = 0\n",
        "#     else:\n",
        "#         pad_h = (size - resized_image.shape[0]) // 2\n",
        "#         pad_w = 0\n",
        "\n",
        "#     padded_image = cv2.copyMakeBorder(resized_image, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT)\n",
        "#     return padded_image\n",
        "\n",
        "# # Function to apply transformations, including the custom isotropic resize\n",
        "# def create_train_transforms(size=380):\n",
        "#     return Compose([\n",
        "#         ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
        "#         GaussNoise(p=0.1),\n",
        "#         GaussianBlur(blur_limit=3, p=0.05),\n",
        "#         HorizontalFlip(),\n",
        "#         Resize(height=size, width=size, always_apply=True),  # Resize to ensure the final output is consistent\n",
        "#         OneOf([RandomBrightnessContrast(), FancyPCA(), HueSaturationValue()], p=0.7),\n",
        "#         ToGray(p=0.2),\n",
        "#         ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=10, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
        "#     ], additional_targets={'image': 'image'})\n",
        "\n",
        "# def apply_isotropic_resize(image, size=380):\n",
        "#     return IsotropicResize(image, size=size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fKeJwIpFYhLb",
        "outputId": "4f02f7b9-e7a6-46c8-8df2-5d367748b64f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame features in train set: (82, 250, 1792)\n",
            "Frame masks in train set: (82, 250)\n"
          ]
        }
      ],
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"filepath\"].values.tolist()\n",
        "    labels = df[\"label\"].values\n",
        "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(\n",
        "            shape=(\n",
        "                1,\n",
        "                MAX_SEQ_LENGTH,\n",
        "            ),\n",
        "            dtype=\"bool\",\n",
        "        )\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :], verbose=0,\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "val_data, val_labels = prepare_all_videos(val_df, \"val\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuWF0skwdJXq"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# class VideoDataGenerator(tf.keras.utils.Sequence):\n",
        "#     def __init__(self, df, root_dir, batch_size, max_seq_length, num_features, label_processor, feature_extractor, shuffle=True, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.df = df\n",
        "#         self.root_dir = root_dir\n",
        "#         self.batch_size = batch_size\n",
        "#         self.max_seq_length = max_seq_length\n",
        "#         self.num_features = num_features\n",
        "#         self.label_processor = label_processor\n",
        "#         self.feature_extractor = feature_extractor\n",
        "#         self.shuffle = shuffle\n",
        "#         self.indices = np.arange(len(self.df))\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         # Number of batches per epoch\n",
        "#         return len(self.df) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         # Generate one batch of data\n",
        "#         batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "#         batch_df = self.df.iloc[batch_indices]\n",
        "#         return self.__data_generation(batch_df)\n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         # Shuffle indices after each epoch if shuffle is set to True\n",
        "#         if self.shuffle:\n",
        "#             np.random.shuffle(self.indices)\n",
        "\n",
        "#     def __data_generation(self, batch_df):\n",
        "#         # Initialize arrays to hold batch data\n",
        "#         batch_size = len(batch_df)\n",
        "#         video_paths = batch_df[\"filepath\"].values.tolist()\n",
        "#         labels = batch_df[\"label\"].values\n",
        "#         labels = keras.ops.convert_to_numpy(self.label_processor(labels[..., None]))\n",
        "\n",
        "#         frame_masks = np.zeros(shape=(batch_size, self.max_seq_length), dtype=\"bool\")\n",
        "#         frame_features = np.zeros(\n",
        "#             shape=(batch_size, self.max_seq_length, self.num_features), dtype=\"float32\"\n",
        "#         )\n",
        "\n",
        "#         for idx, path in enumerate(video_paths):\n",
        "#             frames = load_video(os.path.join(self.root_dir, path))\n",
        "#             frames = frames[None, ...]\n",
        "\n",
        "#             temp_frame_mask = np.zeros(\n",
        "#                 shape=(\n",
        "#                     1,\n",
        "#                     self.max_seq_length,\n",
        "#                 ),\n",
        "#                 dtype=\"bool\",\n",
        "#             )\n",
        "#             temp_frame_features = np.zeros(\n",
        "#                 shape=(1, self.max_seq_length, self.num_features), dtype=\"float32\"\n",
        "#             )\n",
        "\n",
        "#             video_length = frames.shape[1]\n",
        "#             length = min(self.max_seq_length, video_length)\n",
        "#             for j in range(length):\n",
        "#                 temp_frame_features[0, j, :] = self.feature_extractor.predict(\n",
        "#                     frames[:, j, :], verbose=0,\n",
        "#                 )\n",
        "#             temp_frame_mask[0, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "#             frame_features[idx,] = temp_frame_features.squeeze()\n",
        "#             frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "#         return (frame_features, frame_masks), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1M3Ub1WdcXQ"
      },
      "outputs": [],
      "source": [
        "# train_generator = VideoDataGenerator(\n",
        "#     df=train_df,\n",
        "#     root_dir=\"train\",\n",
        "#     batch_size=32,  # Adjust batch size as needed\n",
        "#     max_seq_length=MAX_SEQ_LENGTH,\n",
        "#     num_features=NUM_FEATURES,\n",
        "#     label_processor=label_processor,\n",
        "#     feature_extractor=feature_extractor,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# val_generator = VideoDataGenerator(\n",
        "#     df=val_df,\n",
        "#     root_dir=\"val\",\n",
        "#     batch_size=32,\n",
        "#     max_seq_length=MAX_SEQ_LENGTH,\n",
        "#     num_features=NUM_FEATURES,\n",
        "#     label_processor=label_processor,\n",
        "#     feature_extractor=feature_extractor,\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "# test_generator = VideoDataGenerator(\n",
        "#     df=test_df,\n",
        "#     root_dir=\"test\",\n",
        "#     batch_size=32,\n",
        "#     max_seq_length=MAX_SEQ_LENGTH,\n",
        "#     num_features=NUM_FEATURES,\n",
        "#     label_processor=label_processor,\n",
        "#     feature_extractor=feature_extractor,\n",
        "#     shuffle=False\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuEK25E-mNXc",
        "outputId": "81e6e690-8add-487f-b085-c2758a14e054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[-0.22048397 -0.22533458  0.330482   ... -0.24377686 -0.16279313\n",
            "   -0.22083683]\n",
            "  [-0.22043574 -0.22536124  0.33084404 ... -0.24370903 -0.16319561\n",
            "   -0.2207426 ]\n",
            "  [-0.22526318 -0.21486829  0.34552708 ... -0.23594657 -0.16810413\n",
            "   -0.22859141]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.20833303 -0.14511505  0.5069105  ... -0.17886426 -0.21980815\n",
            "   -0.19552281]\n",
            "  [-0.20816542 -0.14492089  0.5060624  ... -0.17869759 -0.21967046\n",
            "   -0.19546711]\n",
            "  [-0.21387091 -0.15112539  0.46598092 ... -0.18394992 -0.22213219\n",
            "   -0.2017795 ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.23300855 -0.18729949  0.4072029  ... -0.18753105 -0.17101802\n",
            "    0.03543137]\n",
            "  [-0.2331613  -0.18737337  0.40659106 ... -0.18756048 -0.17109431\n",
            "    0.03639219]\n",
            "  [-0.23373008 -0.18831846  0.4283026  ... -0.18969543 -0.177626\n",
            "    0.02751444]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.20520066 -0.16884428  0.9488321  ... -0.18066873 -0.21401903\n",
            "   -0.07346006]\n",
            "  [-0.20585513 -0.16873102  0.9430089  ... -0.18137246 -0.2143403\n",
            "   -0.07360411]\n",
            "  [-0.21071507 -0.1710632   0.8932437  ... -0.18330441 -0.21834075\n",
            "   -0.07233083]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.24686725 -0.21094707 -0.08879582 ... -0.2045998   0.1723552\n",
            "   -0.13791333]\n",
            "  [-0.24293381 -0.21645911 -0.09841906 ... -0.20520322  0.19482057\n",
            "   -0.08678564]\n",
            "  [-0.24559675 -0.20442098 -0.0956322  ... -0.21514596  0.16976008\n",
            "   -0.14106947]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.2129499  -0.14698035  0.3616724  ... -0.21651505 -0.21265426\n",
            "   -0.18618312]\n",
            "  [-0.20953019 -0.14774458  0.38985193 ... -0.21379884 -0.21760039\n",
            "   -0.19880322]\n",
            "  [-0.20759894 -0.1508444   0.4168188  ... -0.2135267  -0.21887806\n",
            "   -0.1825034 ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])\n",
        "# test_data[0]\n",
        "# val_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd0A1JWDQEFo",
        "outputId": "f2245aca-ea8b-423b-858a-520c8dd83785"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[1]\n",
        "# test_data[1]\n",
        "# val_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgTA6uFuTXjT"
      },
      "outputs": [],
      "source": [
        "# train_labels\n",
        "# test_labels\n",
        "# val_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "Cf3NZtyTcZvY",
        "outputId": "fa12cca1-8945-4831-83d8-5012cdcbfd83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.7247"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Layer \"functional_16\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 250, 1792) dtype=float32>]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-e98bdf0c0acf>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-e98bdf0c0acf>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mseq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     history = seq_model.fit(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"functional_16\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 250, 1792) dtype=float32>]"
          ]
        }
      ],
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES), name = \"fname_features\")\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\", name = \"masks\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    # Modify the GRU layers\n",
        "    x = keras.layers.GRU(64, return_sequences=True)(frame_features_input, mask=mask_input)\n",
        "    x = keras.layers.GRU(32)(x)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    x = keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
        "    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"BinaryCrossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Implement gradient clipping\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier/ckpt.weights.h5\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_data=(val_data[0], val_data[1], val_labels),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint, lr_scheduler],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(accuracy)\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXWktU7af-oZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}